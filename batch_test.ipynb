{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: use CONDA Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import ModelNet\n",
    "from torch_geometric.transforms import SamplePoints\n",
    "from torch_geometric.transforms import Compose\n",
    "from torch_geometric.transforms import LinearTransformation\n",
    "from torch_geometric.transforms import GenerateMeshNormals\n",
    "from torch_geometric.transforms import NormalizeScale\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from torch_scatter import scatter_mean\n",
    "import torch_geometric.nn.conv as conv\n",
    "from torch_geometric import utils\n",
    "\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(pos=[216, 3], y=[1], normal=[216, 3])\n"
     ]
    }
   ],
   "source": [
    "num_points = 216\n",
    "batch_size = 32\n",
    "transforms = Compose([SamplePoints(num_points, include_normals=True), NormalizeScale()])\n",
    "dataset_train = ModelNet(root=\"data/ModelNet10\", name='10', train=True, transform=transforms)\n",
    "dataset_test = ModelNet(root=\"data/ModelNet10\", name='10', train=False, transform=transforms)\n",
    "print(dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(pos=[6912, 3], y=[32], normal=[6912, 3], batch=[6912], ptr=[33])\n",
      "tensor([ 0,  0,  0,  ..., 31, 31, 31])\n"
     ]
    }
   ],
   "source": [
    "loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_iter = iter(loader_train)\n",
    "\n",
    "data = loader_iter.next()\n",
    "print(data)\n",
    "print(data.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6912, 6)\n"
     ]
    }
   ],
   "source": [
    "batch = loader_iter.next()\n",
    "batch_pos = batch.pos\n",
    "batch_normal = batch.normal\n",
    "# Concatenating the position and normals\n",
    "batch_X = np.append(batch_pos.numpy(), batch_normal.numpy(), axis=1)\n",
    "print(batch_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 216, 6)\n",
      "(6912, 6)\n",
      "(6912, 6)\n",
      "[[ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " ...\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "batch_X_aux = batch_X.reshape(batch_size, num_points, 6)\n",
    "print(batch_X_aux.shape)\n",
    "batch_X_re = batch_X_aux.reshape([32 * 216, 6])\n",
    "print(batch_X_re.shape)\n",
    "print(batch_X.shape)\n",
    "print(batch_X == batch_X_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetGraph(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates the weighted adjacency matrix 'W'\n",
    "        Taked directly from RGCNN\n",
    "        \"\"\"\n",
    "        super(GetGraph, self).__init__()\n",
    "\n",
    "    def forward(self, point_cloud):\n",
    "        point_cloud_transpose = point_cloud.permute(0, 2, 1)\n",
    "        point_cloud_inner = torch.matmul(point_cloud, point_cloud_transpose)\n",
    "        point_cloud_inner = -2 * point_cloud_inner\n",
    "        point_cloud_square = torch.sum(torch.mul(point_cloud, point_cloud), dim=2, keepdim=True)\n",
    "        point_cloud_square_tranpose = point_cloud_square.permute(0, 2, 1)\n",
    "        adj_matrix = point_cloud_square + point_cloud_inner + point_cloud_square_tranpose\n",
    "        adj_matrix = torch.exp(-adj_matrix)\n",
    "        return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6912, 6)\n",
      "torch.Size([32, 216, 216])\n",
      "torch.Size([6912, 216])\n",
      "torch.Size([6912])\n"
     ]
    }
   ],
   "source": [
    "get_graph = GetGraph()\n",
    "print(batch_X.shape)\n",
    "W = get_graph(torch.tensor(batch_X_aux))\n",
    "print(W.shape)\n",
    "\n",
    "W_reshaped = W.reshape([batch_size * num_points, -1])\n",
    "print(W_reshaped.shape)\n",
    "print(batch.batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.Size([2, 46656])\n",
      "torch.float32\n",
      "torch.Size([1492992])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21706/2721837022.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(torch.tensor(edge[0]).shape)\n",
      "/tmp/ipykernel_21706/2721837022.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index[i] = torch.tensor(edge[0])\n",
      "/tmp/ipykernel_21706/2721837022.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_weight[i] = torch.tensor(edge[1])\n"
     ]
    }
   ],
   "source": [
    "cheb_conv = conv.ChebConv(128, 512, 5)\n",
    "'''\n",
    "for i, graph in enumerate(W):\n",
    "    print(graph.shape)\n",
    "    edge_index, edge_weight = utils.dense_to_sparse(graph)\n",
    "'''\n",
    "edge_index = torch.zeros([32, 2, 46656])\n",
    "edge_weight = torch.zeros([32, 1, 46656])\n",
    "edges = [utils.dense_to_sparse(graph) for graph in W]\n",
    "for i, edge in enumerate(edges):\n",
    "    print(torch.tensor(edge[0]).shape)\n",
    "    edge_index[i] = torch.tensor(edge[0])\n",
    "    edge_weight[i] = torch.tensor(edge[1])\n",
    "edge_index = edge_index.reshape(2, 32 * 46656)\n",
    "edge_weight = edge_weight.reshape(32 * 46656)\n",
    "\n",
    "print(edge_index.dtype)\n",
    "print(edge_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  torch.Size([32, 216, 512])\n",
      "W:    torch.Size([32, 216, 216])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21706/2120053260.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
      "/tmp/ipykernel_21706/2120053260.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "batch_X = torch.tensor(batch_X, dtype=torch.float)\n",
    "X = torch.randn([32, 216, 128])\n",
    "out = cheb_conv(X, edge_index=edge_index, edge_weight=edge_weight, batch=batch.batch)\n",
    "print(\"out: \", out.shape)\n",
    "W = get_graph(out)\n",
    "print(\"W:   \", W.shape)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d70faeaa3fd33be42bd7678075e258256b8dfaa4e00c9780f3a809119fe058e2"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('th_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
